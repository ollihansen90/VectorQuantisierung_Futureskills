{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollihansen90/VectorQuantisierung_Futureskills/blob/main/VecQuant_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWoWw8jZhii"
      },
      "source": [
        "# Kapitel 7: Programmierung des kMeans-Clustering\n",
        "In diesem Notebook geht es um die Implementierung des kMeans-Algorithmus. Hierbei werden sowohl Pattern-by-Pattern- als auch Batch-Learning behandelt.\n",
        "\n",
        "## Setup\n",
        "Im Setup werden drei Punktewolken generiert und danach eingezeichnet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTYlh0AXZhil"
      },
      "outputs": [],
      "source": [
        "# TODO: Auf dem Jupyter-Hub wird die utils.py lokal gespeichert und muss nicht mit wget von Github gezogen werden.\n",
        "!wget -nc -q https://raw.githubusercontent.com/ollihansen90/VectorQuantisierung_Futureskills/main/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sm1seSGZhio"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import setup, zugehoerigkeit, draw, run_experiments\n",
        "\n",
        "data, _ = setup()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(data[:,0], data[:,1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZz4G7ZmZhis"
      },
      "source": [
        "## Pattern-by-Pattern-Learning\n",
        "Beim Pattern-by-Pattern-Learning werden die Datenpunkte durchiteriert und der jeweils nächste Codebookvektor leicht verändert. Die Idee dabei ist sehr ähnlich zum Berechnen des Mittelwerts über das exponentielle Mittel, wie es im Notebook zu Kapitel 3 beschrieben wurde.\n",
        "\n",
        "Im folgenden Codeblock wird der kMeans-Update für Pattern-by-Pattern-Learning implementiert. Für einen beliebigen Punkt aus der Menge der Datenpunkte wird zunächst der am nächsten liegende Codebookvektor gesucht. Im Anschluss wird der Codebookvektor aktualisiert. Eine Lernrate von 0.1 bedeutet hier, dass der Codebookvektor um 10% der Strecke in Richtung gegebenen Punkt verschoben wird."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAxbbHCPZhiu"
      },
      "outputs": [],
      "source": [
        "def kmeans_update_pbp(codebook, point, lernrate=0.1):\n",
        "    cb = codebook.copy()\n",
        "    # finde nächsten Codebookvektor\n",
        "    dist = np.inf\n",
        "    for i in range(len(cb)):\n",
        "        abstand = np.sum((cb[i]-point)**2)**0.5 # Berechne Abstand (hier: euklidischer Abstand)\n",
        "        if abstand<dist: # Wenn der aktuelle Abstand kürzer als der bisher kürzeste ist...\n",
        "            dist = abstand\n",
        "            naechster_index = i\n",
        "    # Update für den nächsten Codebookvektor\n",
        "    cb[naechster_index] = cb[naechster_index]+lernrate*(point-cb[naechster_index])\n",
        "    return cb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMW73BMSZhiv"
      },
      "source": [
        "Nachfolgend kann der Algorithmus für unterschiedliche $k$ getestet werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1feg1swsZhiw"
      },
      "outputs": [],
      "source": [
        "# --- Hier kann das k angepasst werden ---\n",
        "k = 8\n",
        "# ----------------------------------------\n",
        "codebook = np.random.rand(k, 2)*(np.max(data, axis=0)-np.min(data, axis=0))+np.min(data, axis=0)\n",
        "label, error = zugehoerigkeit(codebook, data)\n",
        "draw(codebook, data, title=\"Initialisierung\")\n",
        "print(f\"Fehler bei Initialisierung: {error}\")\n",
        "\n",
        "for i in range(3):\n",
        "    for p in np.random.permutation(data):\n",
        "        codebook = kmeans_update_pbp(codebook, p, lernrate=0.1)\n",
        "    label, error = zugehoerigkeit(codebook, data)\n",
        "    draw(codebook, data, title=f\"Epoche {i+1}\")\n",
        "    print(f\"Fehler in Epoche {i+1}: {error}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgtYFJYcZhiz"
      },
      "source": [
        "## Batch-Learning\n",
        "Das Batch-Learning liefert genau den Updateschritt, wie er in den Videos beschrieben wird: Jeder Codebookvektor wird auf den Schwerpunkt seiner Voronoi-Menge verschoben.\n",
        "\n",
        "Im folgenden Codeblock wird der kMeans-Update für Batch-Learning implementiert. Zunächst werden die Punkte auf die Voronoi-Mengen der jeweiligen Codebookvektor verteilt. Im Anschluss wird der Codebookvektor aktualisiert, indem er auf den Schwerpunkt seiner Voronoi-Menge verschoben wird."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o0O0bInZhi0"
      },
      "outputs": [],
      "source": [
        "def kmeans_update_b(codebook, data):\n",
        "    cb = codebook.copy()\n",
        "\n",
        "    # finde nächsten Codebookvektor\n",
        "    voronoi = np.zeros(len(data))\n",
        "    for i in range(len(data)):\n",
        "        dist = np.inf\n",
        "        for j in range(len(cb)):\n",
        "            abstand = np.sum((cb[j]-data[i])**2)**0.5 # Euklidischer Abstand\n",
        "            if abstand<dist:\n",
        "                dist = abstand\n",
        "                voronoi[i] = j\n",
        "\n",
        "    # Update der Codebookvektoren\n",
        "    for i in range(len(cb)):\n",
        "        # Setze cb auf den Schwerpunkt seiner Voronoi-Menge\n",
        "        voronoimenge = np.arange(len(data))[voronoi==i]\n",
        "        if len(voronoimenge)>0:\n",
        "            # Achtung: Wenn die Voronoi-Menge leer ist, produziert Python hier einen Fehler\n",
        "            cb[i] = np.mean(data[voronoimenge], axis=0)\n",
        "    return cb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LBRA2wXZhi1"
      },
      "source": [
        "Nachfolgend kann der Algorithmus für unterschiedliche $k$ getestet werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1mgsyLDZhi3"
      },
      "outputs": [],
      "source": [
        "# --- Hier kann das k angepasst werden ---\n",
        "k = 8\n",
        "# ----------------------------------------\n",
        "codebook = np.random.rand(k, 2)*(np.max(data, axis=0)-np.min(data, axis=0))+np.min(data, axis=0)\n",
        "label, error = zugehoerigkeit(codebook, data)\n",
        "draw(codebook, data, title=\"Initialisierung\")\n",
        "print(f\"Fehler bei Initialisierung: {error}\")\n",
        "\n",
        "for _ in range(3):\n",
        "    codebook = kmeans_update_b(codebook, data)\n",
        "    _, error = zugehoerigkeit(codebook, data)\n",
        "    draw(codebook, data, title=f\"Epoche {i+1}\")\n",
        "    print(f\"Fehler in Epoche {i+1}: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un5CUBTcZhi4"
      },
      "source": [
        "## Wahl für $k$\n",
        "Zum Schluss stellt sich nun noch die Frage zur Wahl der besten Anzahl an Codebookvektoren im Codebook. Es gibt unterschiedliche andere Ansätze, die dynamisch die Anzahl $k$ im Verlauf der Anpassung bestimmen (z. B. DBSCAN oder OPTICS), der klassische kMeans-Algorithmus muss jedoch vorher genau wissen, wie viele Codebookvektoren existieren.\n",
        "\n",
        "Der einfachste Ansatz wäre es, wenn man sich die Daten einmal anzeigen lässt und dann die Anzahl der Cluster durchzählt. Das funktioniert jedoch leider nur bei sehr niedrigdimensionalen Daten! Bei zwei Dimensionen (und manchmal bei drei) kann das oft durch einen Blick bestimmt werden, doch wie soll das bei zehn Dimensionen gehen? Offensichtlich können zehndimensionale Daten nicht so einfach auf einem zweidimensionalen Bildschirm dargestellt werden.\n",
        "\n",
        "Der Vorgang ist jedoch denkbar einfach: kMeans wird mit unterschiedlichen Werten $k$ initialisiert und danach an die Daten angepasst, bis die Codebookvektoren sich nicht oder nur sehr wenig verändern. Man sagt auch: Das Verfahren *konvergiert*. Zum Schluss werden die Fehler einzelner $k$ verglichen und entschieden, für welches $k$ der Algorithmus die besten Ergebnisse liefert.\n",
        "\n",
        "Im folgenden Codeblock wurde dieses Verfahren einmal implementiert. Bei den Daten handelt es sich um 180 zehndimensionale Punkte, die auf sechs Cluster aufgeteilt sind. Diese Cluster sind jeweils normalverteilt mit unterschiedlichen Mittelwerten und Standardabweichungen. Der kMeans-Algorithmus wird nun mit unterschiedlichen Werten für $k$ initialisiert. Hierbei wurden Werte zwischen 1 und 360 gewählt. Jedes Anpassen wurde fünf Mal mit unterschiedlichen Initialisierungen wiederholt, um eine möglichst gute Schätzung für den mittleren Fehler zu erhalten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqbpkv-KZhi4"
      },
      "outputs": [],
      "source": [
        "d = 10\n",
        "n = 30\n",
        "datalist = []\n",
        "for i in range(6):\n",
        "    scale = 2*np.random.rand()+0.5\n",
        "    offset = np.random.randn(d)\n",
        "    offset /= np.linalg.norm(offset)\n",
        "    data = np.random.randn(n, d)*scale+10*offset\n",
        "    datalist.append(data)\n",
        "data = np.row_stack(datalist)\n",
        "\n",
        "run_experiments(data, n_runs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enkMk6nVZhi5"
      },
      "source": [
        "Es ist gut zu erkennen, wie der Fehler mit steigendem $k$ kleiner wird. Für $k=1$ ist der Fehler vergleichsweise schlecht. Das ist auch nicht verwunderlich: Der kMeans-Algorithmus für den einen Codebookvektor auf den Mittelpunkt der Daten verschieben. Für größerwerdendes $k$ fällt der Fehler stark ab.\n",
        "\n",
        "Es ist gut zu erkennen, dass nach $k=6$ das Abfallen etwas schwächer wird, wenn auch nicht ausbleibt. Das liegt in erster Linie daran, dass die gegebenen Daten aus sechs Clustern bestehen, die für $k=6$ jeweils einen Codebookvektor erhalten.\n",
        "\n",
        "Ab $k=180$ ist der Fehler 0. Auch das ist nicht verwunderlich, denn für $k=180$ gibt es für jeden Datenpunkt einen Codebookvektor. Für $k>180$ gibt es sogar mehrfachbelegte Punkte.\n",
        "\n",
        "Für die Wahl der besten Anzahl der Clusterzentren wird genau so eine Abbildung betrachtet. Sicherlich könnte direkt die Anzahl Punkte als Anzahl Codebookvektoren gewählt werden, nur wäre damit nichts gewonnen. Ein geeignetes $k$ sollte groß genug sein, dass der Fehler hinreichend klein ist, allerdings nicht so groß, dass der Speicheraufwand zu groß wird.\n",
        "\n",
        "In dem Beispiel hier hieße, dass $k=100$ einen Fehler hat, der halb so groß wie der von $k=6$ ist, der Speicheraufwand ist jedoch mehr als 16 Mal so groß. Hier würden wir uns also für $k=6$ entscheiden."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}